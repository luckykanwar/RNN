{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'tinyshakespeare.txt'\n",
    "with open(file_name,'r') as f:\n",
    "    inputTest = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#inputTest = \"Lorem ipsums dolors sit amet, consectetur adipiscing elit. Sed tristique mattis suscipit. Sed suscipit tortor eros, ac feugiat felis sollicitudin et. Pellentesque nec malesuada nisi. Curabitur id neque turpis. In dictum, eros eget dictum semper, elit neque pharetra ex, in aliquam lorem tortor non nunc. Mauris accumsan fermentum ullamcorper. Aenean congue ligula finibus ligula finibus porttitor.Vestibulum finibus consectetur orci, ornare porttitor lorem pharetra in. Pellentesque et nibh ac augue gravida gravida. Vestibulum interdum augue sapien. Duis at placerat sapien, in vestibulum arcu. Pellentesque lacinia diam vel orci faucibus fringilla. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vivamus eu porta dolor. Nulla facilisi. Interdum et malesuada fames ac ante ipsum primis in faucibus. Etiam venenatis dolor vitae est aliquam, a condimentum risus commodo. Nulla vel euismod neque, eu rutrum lectus.Aliquam condimentum aliquet tellus, et bibendum metus fringilla vitae. Sed tempor scelerisque dolor. Etiam laoreet elementum dolor in pharetra. In condimentum felis a cursus semper. Sed id magna posuere, malesuada dolor quis, malesuada orci. Proin ac auctor odio. Praesent vitae commodo turpis. Sed aliquam risus urna, dictum varius massa rhoncus scelerisque. Mauris nec tempus dui, porttitor varius risus. Phasellus nec arcu facilisis, lobortis erat luctus, faucibus erat. Sed velit quam, facilisis ut egestas in, sodales ut quam. Pellentesque aliquam eu neque sed fermentum. Quisque sed mattis arcu. Aliquam mattis eros a lacus semper porta nec ac nunc.Suspendisse felis lacus, fringilla vel pellentesque id, bibendum nec lorem. Mauris fermentum nisl sed purus blandit facilisis. Sed vulputate vitae velit rhoncus porta. Sed feugiat faucibus risus at ultrices. Suspendisse pellentesque tincidunt justo ac finibus. Donec volutpat ultricies arcu non facilisis. Aenean eget felis rutrum, laoreet nulla ut, rhoncus nulla. Curabitur scelerisque, tortor nec commodo aliquam, sem augue vulputate odio, non vehicula neque nulla quis dui. Morbi maximus lacus in libero dignissim laoreet. Duis at iaculis nibh, vel consectetur eros. Vivamus tincidunt mauris sit amet orci vestibulum luctus. Quisque tempor elit felis. Phasellus dictum faucibus est, quis feugiat nisl lobortis nec. Nulla facilisi. Donec eu sapien nec purus ultricies eleifend a vitae metus.Suspendisse ultricies tristique posuere. Proin convallis, tellus sed lacinia tincidunt, mi lectus fringilla ante, eu pretium nibh lorem non est. In scelerisque massa vel sodales lobortis. Mauris hendrerit libero ut velit dictum, quis sollicitudin arcu sodales. Ut ipsum diam, gravida eu rhoncus in, venenatis id nisl. Nulla sollicitudin feugiat tempus. Duis eget ligula aliquam arcu tristique molestie. In lacinia quam nisl. Cras finibus orci ac ex gravida, mollis varius odio eleifend. Curabitur et tincidunt lectus. Proin feugiat nibh turpis, non dignissim purus condimentum ut. Maecenas sed rhoncus nunc. Morbi fringilla laoreet neque, quis varius sapien\"\n",
    "data_size = len(inputTest)\n",
    "print(data_size)\n",
    "individualChars = list(set(inputTest))\n",
    "vocab_size = len(individualChars)\n",
    "print(vocab_size)\n",
    "num_hidden_units = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert input into batches of seq len 15\n",
    "char_to_ix = {char: idx for idx, char in enumerate(individualChars)}\n",
    "ix_to_char = {idx : char for idx, char in enumerate(individualChars)}\n",
    "seq_len = 15\n",
    "length = 0\n",
    "data_set = []\n",
    "word = []\n",
    "while(length < data_size):\n",
    "    #print(\"line : \", inputTest[length])\n",
    "    charArray = np.zeros((vocab_size,1))\n",
    "    charArray[char_to_ix[inputTest[length]]] = 1\n",
    "    word.append(charArray)\n",
    "    length += 1\n",
    "\n",
    "word = np.array(word)\n",
    "print(word.shape)\n",
    "\n",
    "batches_train_x = []\n",
    "batches_train_y = []\n",
    "length = 0\n",
    "while length<word.shape[0]:\n",
    "    #print(word[length:(length+seq_len-2)])\n",
    "    batches_train_x.append(word[length:(length+seq_len)])\n",
    "    batches_train_y.append(word[length+1:(length+seq_len+1)])\n",
    "    length = length + seq_len\n",
    "\n",
    "print(len(batches_train_x[204]))\n",
    "print(len(batches_train_y[204])) # 205  batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65,)\n"
     ]
    }
   ],
   "source": [
    "# Convert input into batches of seq len 15\n",
    "char_to_ix = {char: idx for idx, char in enumerate(individualChars)}\n",
    "ix_to_char = {idx : char for idx, char in enumerate(individualChars)}\n",
    "seq_len = 32\n",
    "length = 0\n",
    "data_set = []\n",
    "word = []\n",
    "while(length < data_size):\n",
    "    #print(\"line : \", inputTest[length])\n",
    "    charArray = np.zeros((vocab_size,))\n",
    "    charArray[char_to_ix[inputTest[length]]] = 1\n",
    "    word.append(charArray)\n",
    "    length += 1\n",
    "\n",
    "#word = np.array(word)\n",
    "print(word[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34856\n",
      "34856\n"
     ]
    }
   ],
   "source": [
    "batches_train_x = []\n",
    "batches_train_y = []\n",
    "length = 0\n",
    "#while length+seq_len+1<word.shape[0]:\n",
    "while length+seq_len+1<len(word):\n",
    "    #print(word[length:(length+seq_len-2)])\n",
    "    batches_train_x.append(word[length:(length+seq_len)])\n",
    "    batches_train_y.append(word[length+1:(length+seq_len+1)])\n",
    "    length = length + seq_len\n",
    "    #print(\"length\", length)\n",
    "\n",
    "print(len(batches_train_x))\n",
    "print(len(batches_train_y))\n",
    "num_of_batches = len(batches_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34856, 32, 65)\n",
      "float64\n",
      "(34856, 32, 65)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "#batches_train_x = np.array(batches_train_x, dtype='float32')\n",
    "batches_train_x = np.array(batches_train_x)\n",
    "batches_train_x.reshape([num_of_batches,vocab_size,seq_len])\n",
    "\n",
    "batches_train_y = np.array(batches_train_y)\n",
    "batches_train_y.reshape([num_of_batches,vocab_size,seq_len])\n",
    "\n",
    "print(batches_train_x.shape)\n",
    "print(batches_train_x.dtype)\n",
    "\n",
    "print(batches_train_y.shape)\n",
    "print(batches_train_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word = np.array(word)\n",
    "word = word.reshape(204,15,38)\n",
    "one_word = word[0]\n",
    "for characters in one_word:\n",
    "    print(characters)\n",
    "    print(characters.nonzero())\n",
    "    one_char = (characters.nonzero()[0])[0]\n",
    "    print(ix_to_char[one_char])\n",
    "'''\n",
    "print(one_word.shape)\n",
    "    print(one_word[0])\n",
    "    print(one_word[0].nonzero()[0])\n",
    "    test = one_word[0].nonzero()[0]\n",
    "    print(test[0])\n",
    "    print(ix_to_char)\n",
    "    print(ix_to_char[test[0]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = tf.placeholder(name='X', dtype=tf.float64, shape= [None, seq_len, vocab_size])\n",
    "#Y = tf.placeholder(name='Y', dtype=tf.float64, shape= [None, vocab_size])\n",
    "X = tf.placeholder(name='X', dtype=tf.float64, shape= [None, vocab_size, 1])\n",
    "Y = tf.placeholder(name='Y', dtype=tf.float64, shape= [None, vocab_size])\n",
    "\n",
    "#inputs=tf.unstack(X ,seq_len,1)\n",
    "#inpt=tf.unstack(X,seq_len,0)\n",
    "#print(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'out_weights_3:0' shape=(128, 65) dtype=float64_ref>\n",
      "<tf.Variable 'out_bias_3:0' shape=(65,) dtype=float64_ref>\n"
     ]
    }
   ],
   "source": [
    "#word = []\n",
    "#a = np.zeros((38,1))\n",
    "#b = np.zeros((38,1))\n",
    "#a[char_to_idx['a']] = 1\n",
    "#b[char_to_idx['t']] = 1\n",
    "#word.append(a)\n",
    "#word.append(b)\n",
    "#print(word)\n",
    "\n",
    "\n",
    "#for data in data_set:\n",
    "#    print(data)\n",
    "#    for char in data:\n",
    "#        charArray = np.zeros((vocab_size,1))\n",
    "#        charArray[char_to_ix[char]] = 1\n",
    "#        word.append(charArray)\n",
    "#final_data_batch = np.array(word)\n",
    "\n",
    "#final_data_batch = final_data_batch.reshape(204,seq_len,vocab_size)\n",
    "#print(final_data_batch.shape)\n",
    "\n",
    "out_weights=tf.Variable(tf.random_normal([num_hidden_units,vocab_size], dtype=tf.float64), name=\"out_weights\", dtype=tf.float64)\n",
    "out_bias=tf.Variable(tf.random_normal([vocab_size], dtype=tf.float64), name=\"out_bias\", dtype=tf.float64)\n",
    "#a = tf.random_normal([num_hidden_units,vocab_size], dtype=tf.float64)\n",
    "#out_weights = tf.Variable(a)\n",
    "\n",
    "print(out_weights)\n",
    "\n",
    "print(out_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out_weights1=tf.Variable(tf.random_normal([128,38]))\n",
    "out_bias1=tf.Variable(tf.random_normal([38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6f747edcc0>\n"
     ]
    }
   ],
   "source": [
    "lstm = tf.nn.rnn_cell.LSTMCell(num_hidden_units)\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, _ = tf.nn.dynamic_rnn(lstm, X, dtype=tf.float64)\n",
    "#outputs, _ = tf.nn.static_rnn(lstm,inputs, dtype=tf.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose_3:0\", shape=(65, ?, 128), dtype=float64)\n",
      "Tensor(\"Gather_3:0\", shape=(?, 128), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "output_inter = tf.transpose(outputs, [1, 0, 2])\n",
    "print(output_inter)\n",
    "last = tf.gather(output_inter, int(output_inter.get_shape()[0]) - 1)\n",
    "#last1 = tf.gather(output_inter, int(output_inter.get_shape()[0]))\n",
    "print(last)\n",
    "#print(last1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore this part for now\n",
    "def RNN(x, num_hidden_units):\n",
    "    lstm = rnn.BasicLSTMCell(num_hidden_units)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm, X, dtype=tf.float32)\n",
    "    print(outputs)\n",
    "    return tf.matmul(outputs, out_weights, name=\"multiplyThese\") + out_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#with tf.variable_scope(\"rnn\"):\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(num_hidden_units)\n",
    "    print(lstm)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm,X, dtype=tf.float64)\n",
    "    #outputs, _ = tf.nn.static_rnn(lstm,inputs, dtype=tf.float64)\n",
    "    print(outputs)\n",
    "    output_inter = tf.transpose(outputs, [1, 0, 2])\n",
    "    print(output_inter)\n",
    "    last = tf.gather(output_inter, int(output_inter.get_shape()[0]) - 1)\n",
    "    last1 = tf.gather(output_inter, int(output_inter.get_shape()[0]))\n",
    "    print(last)\n",
    "    print(last1)\n",
    "    tf.get_variable_scope().reuse_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.variable_scope(\"scope1\", reuse = None):\n",
    "    \n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(num_hidden_units)\n",
    "    print(lstm)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm,X, dtype=tf.float64)\n",
    "    #outputs, _ = tf.nn.static_rnn(lstm,inputs, dtype=tf.float64)\n",
    "    print(outputs)\n",
    "    output_inter = tf.transpose(outputs, [1, 0, 2])\n",
    "    print(output_inter)\n",
    "    last = tf.gather(output_inter, int(output_inter.get_shape()[0]) - 1)\n",
    "    last1 = tf.gather(output_inter, int(output_inter.get_shape()[0]))\n",
    "    print(last)\n",
    "    print(last1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_9:0\", shape=(?, 65), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#logits = RNN(X, num_hidden_units)\n",
    "prediction = tf.matmul(last, out_weights, name = \"softmax\") + out_bias\n",
    "print(prediction)\n",
    "logits = tf.nn.softmax(prediction)\n",
    "index = tf.argmax(logits,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_16:0\", shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=Y))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.get_variable_scope().reuse_variables()\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = tf.not_equal(tf.argmax(Y, 1), tf.argmax(prediction, 1))\n",
    "error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "correct_prediction=tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  0\n",
      "For iter  0\n",
      "Accuracy  0.28125\n",
      "Loss  2.257967968671072\n",
      "__________________\n",
      "Epoch -  1\n",
      "Epoch -  2\n",
      "Epoch -  3\n",
      "Epoch -  4\n",
      "Epoch -  5\n",
      "Epoch -  6\n",
      "Epoch -  7\n",
      "Epoch -  8\n",
      "Epoch -  9\n",
      "Epoch -  10\n",
      "For iter  10\n",
      "Accuracy  0.375\n",
      "Loss  2.0723994372611294\n",
      "__________________\n",
      "Epoch -  11\n",
      "Epoch -  12\n",
      "Epoch -  13\n",
      "Epoch -  14\n",
      "Epoch -  15\n",
      "Epoch -  16\n",
      "Epoch -  17\n",
      "Epoch -  18\n",
      "Epoch -  19\n",
      "Epoch -  20\n",
      "For iter  20\n",
      "Accuracy  0.34375\n",
      "Loss  2.0472793368519504\n",
      "__________________\n",
      "Epoch -  21\n",
      "Epoch -  22\n",
      "Epoch -  23\n",
      "Epoch -  24\n",
      "Epoch -  25\n",
      "Epoch -  26\n",
      "Epoch -  27\n",
      "Epoch -  28\n",
      "Epoch -  29\n",
      "Epoch -  30\n",
      "For iter  30\n",
      "Accuracy  0.375\n",
      "Loss  2.134503851064947\n",
      "__________________\n",
      "Epoch -  31\n",
      "Epoch -  32\n",
      "Epoch -  33\n",
      "Epoch -  34\n",
      "Epoch -  35\n",
      "Epoch -  36\n",
      "Epoch -  37\n",
      "Epoch -  38\n",
      "Epoch -  39\n",
      "Epoch -  40\n",
      "For iter  40\n",
      "Accuracy  0.34375\n",
      "Loss  2.1396873017279434\n",
      "__________________\n",
      "Epoch -  41\n",
      "Epoch -  42\n",
      "Epoch -  43\n",
      "Epoch -  44\n",
      "Epoch -  45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-d9c6b2bce867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m#print(input_x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m#print(input_y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch - \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    generated_words = []\n",
    "    for i in range(1000):\n",
    "        for j in range(num_of_batches):\n",
    "            #print(\"j : \", j)\n",
    "            #if j == 0:\n",
    "                #print(\"input: \", j, batches_train_x[j])\n",
    "                #print(\"label: \", j, batches_train_y[j])\n",
    "            input_x = np.array(batches_train_x[j])\n",
    "            input_y = np.array(batches_train_y[j])\n",
    "            input_x = input_x.reshape(seq_len,vocab_size,1)\n",
    "            input_y = input_y.reshape(seq_len,vocab_size)\n",
    "            #print(input_x.shape)\n",
    "            #print(input_y.shape)\n",
    "            sess.run(optimize, feed_dict={X:input_x, Y:input_y})\n",
    "        print(\"Epoch - \",str(i))\n",
    "        \n",
    "        if i%10==0:\n",
    "            acc=sess.run(accuracy,feed_dict={X:input_x,Y:input_y})\n",
    "            los=sess.run(loss,feed_dict={X:input_x,Y:input_y})\n",
    "            print(\"For iter \",i)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "            \n",
    "        if i%2 == 0:\n",
    "            sample = np.zeros((vocab_size,1))\n",
    "            sample[random.randint(0,vocab_size)] = 1\n",
    "            sample = sample.reshape(1,vocab_size,1)\n",
    "            \n",
    "            for k in range(10):\n",
    "                resulting_logits = sess.run(logits, feed_dict={X:sample})\n",
    "                resulting_index = sess.run(index, feed_dict={X:sample})\n",
    "                \n",
    "                #print(resulting_logits)\n",
    "                #print(resulting_index[0])\n",
    "                generated_words.append(ix_to_char[resulting_index[0]]) \n",
    "                #print(generated_words)\n",
    "    print(generated_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
